{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e212824-4e96-42d0-a5f3-8e4c41bcb5a5",
   "metadata": {},
   "source": [
    "Pekiştirmeli öğrenme (Reinforcement Learning, RL), bir yapay zekâ modelinin, belirli bir ortamda en iyi eylemleri öğrenerek belirli bir hedefe ulaşmasını sağlayan bir makine öğrenmesi yöntemidir. Bu yöntemde, model, bir dizi durum (state), eylem (action) ve ödül (reward) üzerinden çalışır. Pekiştirmeli öğrenmede temel olarak şu adımlar izlenir:\n",
    "\n",
    "1. **Durum (State):** Çevrenin mevcut durumu, model tarafından algılanır.\n",
    "2. **Eylem (Action):** Model, mevcut durumdan bir eylem seçer.\n",
    "3. **Ödül (Reward):** Eylemin sonucunda bir ödül veya ceza alır.\n",
    "4. **Geri Bildirim (Feedback):** Alınan ödül veya ceza ile model, eylemin ne kadar başarılı olduğunu öğrenir.\n",
    "5. **Politika (Policy):** Model, bu geri bildirimleri kullanarak gelecekteki durumlar için daha iyi eylemler seçmeye çalışır.\n",
    "\n",
    "### Q-Oğrenme Algoritması\n",
    "\n",
    "Modelin amacı, uzun vadede en yüksek toplam ödülü elde edecek şekilde davranışlarını optimize etmektir. Pekiştirmeli öğrenme, deneme-yanılma yoluyla öğrenme sürecini içerir ve genellikle karmaşık ve dinamik ortamlarda kullanılır. Bu yöntemin uygulama alanları arasında robotik, oyunlar, finansal modelleme ve özerk sistemler yer alır.\n",
    "\n",
    "\n",
    "Q-öğrenme algoritması, pekiştirmeli öğrenme (Reinforcement Learning, RL) alanında yaygın olarak kullanılan bir yöntemdir. Bir ajanın (agent) bir ortamda nasıl davranması gerektiğini öğrenmesine yardımcı olur. Bu öğrenme süreci, ödül (reward) sinyallerine dayanır ve amaç, ajan için uzun vadede en yüksek toplam ödülü elde edecek bir politika (policy) geliştirmektir.\n",
    "\n",
    "Q-öğrenme algoritmasının ana bileşenleri ve işleyişi şu şekildedir:\n",
    "\n",
    "### 1. Durum (State) ve Eylem (Action)\n",
    "- **Durum (State, \\( s \\))**: Ajanın içinde bulunduğu mevcut çevresel durum.\n",
    "- **Eylem (Action, \\( a \\))**: Ajanın mevcut durumda yapabileceği hareketler veya kararlar.\n",
    "\n",
    "### 2. Q-Tablosu (Q-Table)\n",
    "- Q-tablosu, her durum-eylem çifti için bir Q-değerini (eylem değeri) saklayan bir tablodur. Q-değeri, belirli bir durumda belirli bir eylemi seçmenin beklenen ödülüdür.\n",
    "- Q-tablosu başlangıçta sıfırlarla başlatılır ve zamanla güncellenir.\n",
    "\n",
    "### 3. Güncelleme Kuralı\n",
    "Q-öğrenme, Bellman denklemini kullanarak Q-değerlerini iteratif olarak günceller. Güncelleme kuralı şu şekildedir:\n",
    "\n",
    "$[ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right] ]$\n",
    "\n",
    "Burada:\n",
    "- \\( $\\alpha$ \\) (öğrenme oranı): Q-değerinin ne kadarının yeni bilgiyle güncelleneceğini belirler.\n",
    "- \\( r \\) (ödül): Ajanın eylem sonucunda aldığı ödül.\n",
    "- \\( $\\gamma$ \\) (indirim faktörü): Gelecekteki ödüllerin ne kadar önemli olduğunu belirler.\n",
    "- \\( s \\): Eylem sonrası yeni durum.\n",
    "- \\( $\\max_{a'}$ $Q(s', a')$ \\): Yeni durumda seçilebilecek en iyi eylemin Q-değeri.\n",
    "\n",
    "### 4. Keşif ve Sömürü (Exploration and Exploitation)\n",
    "- Ajanın, Q-tablosunu güncellerken hem çevresini keşfetmesi (exploration) hem de öğrendiği bilgiyi kullanarak en iyi eylemleri seçmesi (exploitation) gerekir.\n",
    "- **Epsilon-greedy politika**: Ajan, epsilon (\\( \\epsilon \\)) olasılıkla rastgele bir eylem seçer (keşif), ve \\( 1 - \\epsilon \\) olasılıkla en yüksek Q-değerine sahip eylemi seçer (sömürü).\n",
    "\n",
    "### 5. Algoritmanın Adımları\n",
    "1. **Başlatma**: Q-tablosunu sıfırlarla başlat.\n",
    "2. **Episod başlatma**: Ortamı resetleyerek başlangıç durumuna geç.\n",
    "3. **Adımları tekrarla**:\n",
    "   - Mevcut durumda bir eylem seç (epsilon-greedy politika kullanarak).\n",
    "   - Eylemi uygula ve yeni durumu ve ödülü al.\n",
    "   - Q-tablosunu yukarıdaki güncelleme kuralı ile güncelle.\n",
    "   - Yeni duruma geç.\n",
    "   - Eğer hedefe ulaşıldıysa veya maksimum adım sayısına ulaşıldıysa bu epizodu bitir.\n",
    "4. **Epsilon'u azalt**: Daha fazla bilgi topladıkça keşif oranını azalt (decay rate ile).\n",
    "5. **Episodları tekrarla**: Belirli bir sayıda (örneğin 10.000) epizod tamamlanana kadar adımları tekrarla.\n",
    "\n",
    "Q-öğrenme algoritması, doğru şekilde uygulandığında ajan, en iyi eylemleri öğrenir ve ödülünü maksimize edecek bir politika geliştirir. Bu algoritma, özellikle çevrimdışı öğrenme ve geniş durum-eylem uzaylarına sahip sorunlarda etkili bir yöntemdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f0b305-994e-42ea-8923-5599d9bc776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'gymnasium[all]' &>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf32b97-498b-4e71-954e-586549562084",
   "metadata": {},
   "source": [
    "\n",
    "### \"CartPole-v1\" Ortamı Örneği\n",
    "\n",
    "\"CartPole-v1\" ortamında, `observation` çıktısı dört değerden oluşur:\n",
    "1. **Cart Position (aracın pozisyonu)**: Arabacın yatay eksendeki konumu.\n",
    "2. **Cart Velocity (aracın hızı)**: Arabacın yatay eksendeki hızı.\n",
    "3. **Pole Angle (direğin açısı)**: Direğin dikeyden sapma açısı.\n",
    "4. **Pole Velocity at Tip (direğin ucundaki hız)**: Direğin ucunun açısal hızı.\n",
    "\n",
    "### Kod Örneği ve Çıktı\n",
    "\n",
    "```python\n",
    "import gym\n",
    "\n",
    "# CartPole ortamını oluştur\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Ortamı başlat ve ilk gözlemi al\n",
    "observation = env.reset()\n",
    "print(\"İlk gözlem:\", observation)\n",
    "\n",
    "# Rastgele bir eylem gerçekleştir ve yeni gözlemi al\n",
    "action = env.action_space.sample()\n",
    "observation, reward, done, info = env.step(action)\n",
    "print(\"Yeni gözlem:\", observation)\n",
    "```\n",
    "\n",
    "### Olası Çıktı\n",
    "\n",
    "```plaintext\n",
    "İlk gözlem: [-0.03225139  0.02965271 -0.02572898 -0.03329633]\n",
    "Yeni gözlem: [-0.03165833 -0.16500978 -0.02639591  0.24774924]\n",
    "```\n",
    "\n",
    "Bu çıktı, `CartPole-v1` ortamında bir eylem gerçekleştirdikten sonra alınan gözlemi göstermektedir. Dört değer de sırasıyla arabacın pozisyonunu, hızını, direğin açısını ve direğin ucundaki hızı temsil eder.\n",
    "\n",
    "### Gözlem Alanlarının Açıklaması\n",
    "\n",
    "- **Box**: Sürekli değerler içeren bir uzayı temsil eder. `CartPole-v1` ortamında, gözlem alanı `Box(-inf, inf, (4,), dtype=float32)` olarak tanımlanır, yani dört boyutlu sürekli bir uzaydır.\n",
    "  \n",
    "  ```python\n",
    "  import gym\n",
    "  env = gym.make('CartPole-v1')\n",
    "  print(env.observation_space)\n",
    "  # Output: Box(-inf, inf, (4,), float32)\n",
    "  ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b6a94-81f6-48d4-a986-dec9adece0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "ortam = gym.make('CartPole-v1',render_mode='human')\n",
    "ortam_durumu = ortam.reset()\n",
    "ortam_durumu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428a9176-0304-4f27-aed3-94f8366d86b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint(ortam):\n",
    "    print(f\"Aracin pozisyonu : {ortam[0]}\")\n",
    "    print(f\"Aracin hizi : {ortam[1]}\")\n",
    "    print(f\"Diregin acisi : {ortam[2]}\")\n",
    "    print(f\"Diregin ucunun hizi : {ortam[3]}\")\n",
    "\n",
    "pprint(ortam_durumu[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c71c41-ec5b-4d2a-8988-0d0e76660040",
   "metadata": {},
   "outputs": [],
   "source": [
    "ortam_durumu, odul, bittimi, kesildimi, bilgi = ortam.step(1)\n",
    "pprint(ortam_durumu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2880e18-e8b7-4e54-9b43-491f3f554851",
   "metadata": {},
   "outputs": [],
   "source": [
    "ortam.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3cbbd2-7036-4057-92fa-3076be6f6889",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Aşağıdaki kod, CartPole ortamını kullanarak bir ajanı rastgele hareketlerle çalıştırır:\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# CartPole ortamını başlat\n",
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "obs = env.reset()  # Ortamı sıfırla\n",
    "\n",
    "# 1000 adım için simülasyonu çalıştır\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # Rastgele bir aksiyon seç\n",
    "    obs, reward, done, truncated, info = env.step(action)  # Aksiyonu uygula\n",
    "    env.render()  # Ortamı görselleştir\n",
    "    if done:\n",
    "        obs = env.reset()  # Ortam sona erdiğinde sıfırla\n",
    "\n",
    "env.close()\n",
    "```\n",
    "\n",
    "### 3. Q-Learning İle CartPole Ajanı Eğitme\n",
    "\n",
    "Q-Learning algoritmasını kullanarak ajanı eğitmek için daha karmaşık bir kod yazmamız gerekiyor. İşte temel bir Q-Learning algoritması ile CartPole ajanını eğiten bir örnek:\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Ortamı başlat\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Q-table oluştur\n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "# Hiperparametreler\n",
    "alpha = 0.1  # Öğrenme oranı\n",
    "gamma = 0.99  # Gelecekteki ödüllerin indirim oranı\n",
    "epsilon = 0.1  # Keşif oranı\n",
    "num_episodes = 1000  # Eğitim için bölüm sayısı\n",
    "\n",
    "def get_discrete_state(state, bins=(10, 10, 10, 10)):\n",
    "    discrete_state = tuple(np.digitize(s, np.linspace(-1, 1, num)) for s, num in zip(state, bins))\n",
    "    return discrete_state\n",
    "\n",
    "# Eğitim döngüsü\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = get_discrete_state(state)\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state])\n",
    "\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        next_state = get_discrete_state(next_state)\n",
    "\n",
    "        # Q-value güncellemesi\n",
    "        best_next_action = np.argmax(Q[next_state])\n",
    "        td_target = reward + gamma * Q[next_state][best_next_action]\n",
    "        td_delta = td_target - Q[state][action]\n",
    "        Q[state][action] += alpha * td_delta\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Eğitimden sonra, ajanı test edelim\n",
    "state, _ = env.reset()\n",
    "state = get_discrete_state(state)\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(Q[state])\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    next_state = get_discrete_state(next_state)\n",
    "    env.render()\n",
    "    state = next_state\n",
    "\n",
    "env.close()\n",
    "```\n",
    "\n",
    "Bu kod, basit bir Q-Learning algoritması ile CartPole ajanını eğitir ve ardından eğitilen ajanı test eder. Q-Learning algoritması, öğrenme oranı (`alpha`), indirim oranı (`gamma`) ve keşif oranı (`epsilon`) gibi hiperparametreler kullanarak ajanı eğitir.\n",
    "\n",
    "Eğer daha gelişmiş RL algoritmalarını (örneğin, DQN, A2C, PPO) kullanmak isterseniz, `stable-baselines3` gibi kütüphaneleri kullanabilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe39fdf5-327d-4d6a-a051-4489afaa083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Ortamı başlat\n",
    "env = gym.make('CartPole-v1',render_mode='human')\n",
    "\n",
    "# Q-table oluştur\n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "# Hiperparametreler\n",
    "alpha = 0.1  # Öğrenme oranı\n",
    "gamma = 0.99  # Gelecekteki ödüllerin indirim oranı\n",
    "epsilon = 0.1  # Keşif oranı\n",
    "num_episodes = 1000  # Eğitim için bölüm sayısı\n",
    "\n",
    "def get_discrete_state(state, bins=(10, 10, 10, 10)):\n",
    "    discrete_state = tuple(np.digitize(s, np.linspace(-1, 1, num)) for s, num in zip(state, bins))\n",
    "    return discrete_state\n",
    "\n",
    "# Eğitim döngüsü\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = get_discrete_state(state)\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state])\n",
    "\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        next_state = get_discrete_state(next_state)\n",
    "\n",
    "        # Q-value güncellemesi\n",
    "        best_next_action = np.argmax(Q[next_state])\n",
    "        td_target = reward + gamma * Q[next_state][best_next_action]\n",
    "        td_delta = td_target - Q[state][action]\n",
    "        Q[state][action] += alpha * td_delta\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Eğitimden sonra, ajanı test edelim\n",
    "state, _ = env.reset()\n",
    "state = get_discrete_state(state)\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(Q[state])\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    next_state = get_discrete_state(next_state)\n",
    "    env.render()\n",
    "    state = next_state\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
